[{"title":"linux","url":"/2024/07/28/%E6%8A%80%E6%9C%AF/Linux%E7%9F%A5%E8%AF%86/","content":"gitgit clone下来的仓库就可以 git status查看记录\n对于github 私有仓库，使用PAT密钥方式\ngit clone 有时需要代理 GitHub Proxy 最新地址发布\ngit config --local user.emailgit config --local user.namegit config --local user.email &quot;lingshi321@gmail.com&quot;//设置信息，保证上传时显示头像添加github网址代理XXX就是正常git网址：https://github.com/ydshi0/LoongServe.gitgit remote set-url origin https://ghfast.top/XXXgit remote -vgit switch -c v1_pd_yhc_debug origin/v1_pd_yhc_debug// 创建本地分支，同时追踪远程分支git pull  //拉取代码git push  //上传代码git status git add .git commit -m &quot;&quot;git fetch origin git branch -rgit branch -vvgit stash  //暂存git pullgit stash popgit config core.fileMode false      //忽略权限改变\nVScode插件：Git History 查看提交历史\n网络Clash Vergehttp代理\n$env:HTTP_PROXY=&quot;http://127.0.0.1:7897&quot;; $env:HTTPS_PROXY=&quot;http://127.0.0.1:7897&quot;\nssh代理\n需要下载Nmap，然后打开socks协议端口。\nssh代理之后，发布博客就很流畅，而且还能连接外国的H100集群。\nHost github.com    HostName github.com    User git    Port 22    IdentityFile C:\\Users\\11762/.ssh/id_rsa    ProxyCommand ncat --proxy 127.0.0.1:50801 --proxy-type socks5 %h 22\n大模型下载正常就用Hugging Face，下面是镜像\nHF-Mirror\npip install -U huggingface_hubexport HF_ENDPOINT=https://hf-mirror.comhuggingface-cli download bigscience/bloom-560m --local-dir bloom-560m\n首页 · 魔搭社区 \nmodelscope download --model modelscope/Llama-2-13b-ms --local_dir Llama-2-13b\ntmux可以让命令保持在后台运行\ntmux new -s nametmux attach -t nametmux lstmux kill-session -t 0//鼠标上下滚动Ctrl + b 输入冒号: set -g mouse on//退出tmuxCtrl + b  d\n进程ps查找进程，grep搜索。\n容器里面看不到外面的进程。\nps aux | grep bashpkill -x python3pkill -x bash   \nctrl + C 中断， ctrl + Z 暂停，fg调到前台\n命令pwdrm -rf XXX        //只用绝对路径防止出错chmod -R 777 XXX   //设置权限 historywhich python\nC++环境VScode + WSL 搭建Linux环境，不要在windows环境折腾！\nWSL 可以使用万能头文件 #include  ，VScode也有代码补全,  写C++很方便。\n其他pip换源，apt-get换源\n","tags":["技术","linux"]},{"title":"cuda知识","url":"/2025/02/25/%E6%8A%80%E6%9C%AF/cuda%E7%9F%A5%E8%AF%86/","content":"GPU参数SM（Streaming Multiprocessor）GPU的核心计算单元，一个GPU含有多个SM\n每个SM包含：\n\nCUDA核心（SP, Streaming Processor）：最基本的计算单元，执行浮点和整数运算\n共享内存（Shared Memory）：SM内线程块可共享的高速内存\n寄存器（Registers）：为每个线程分配私有存储\n调度器（Warp Scheduler）：管理线程束（Warp）的执行\n\nThread、Block、GridBlock：包含多个Thread，同一Block内的线程共享SM资源（如Shared Memory）\nGrid：包含多个block\nWarp（线程束）：GPU的最小调度单位，固定包含32个线程\n每个Block最多包含1024个Thread（受硬件限制）\nBlock大小建议设为32的倍数（与Warp对齐，避免资源浪费）\n每个 Block 会被分配到一个 SM 上执行，一个 SM 可以同时执行多个 Block\nCUDA核函数使用__global__修饰，通过&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;指定执行配置\ngrid，block 支持1D/2D/3D索引（dim3），用索引来识别对应的计算任务\n__global__ void vectorAdd(const float *A, const float *B, float *C, int N) &#123;    int i = blockIdx.x * blockDim.x + threadIdx.x;    if (i &lt; N) &#123;        C[i] = A[i] + B[i];    &#125;&#125;vectorAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);\n在CUDA核函数中，第一个参数通常是 output\nGPU结构INT32 ：32位整型运算的器件\nFP32 : 32位浮点型计算 \nTensor Core: 矩阵运算\n\n资料\nNvidia 显卡架构详解 – Twisted Meadows （比较GPU的方法）\n","tags":["技术","cuda"]},{"title":"多机训练","url":"/2024/01/03/%E6%8A%80%E6%9C%AF/%E5%A4%9A%E6%9C%BA%E8%AE%AD%E7%BB%83/","content":"多机训练一个节点NODE_RANK=0，另一个节点NODE_RANK=1\n设置网卡，MASTER_ADDR (要写成网址,不能写成localhost ！)\n两边手动启动就不用ssh免密登录\n#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1export GLOO_SOCKET_IFNAME=ens5f0export TP_SOCKET_IFNAME=ens5f0GPUS_PER_NODE=4# Change for multinode configMASTER_ADDR=192.168.1.101MASTER_PORT=13466NUM_NODES=2NODE_RANK=0WORLD_SIZE=$(($GPUS_PER_NODE*$NUM_NODES))VOCAB_FILE=./need/gpt2-vocab.jsonMERGE_FILE=./need/gpt2-merges.txtDISTRIBUTED_ARGS=(    --nproc_per_node $GPUS_PER_NODE     --nnodes $NUM_NODES     --master_addr $MASTER_ADDR     --master_port $MASTER_PORT    --node_rank $NODE_RANK )GPT_MODEL_ARGS=(    --num-layers 8    --hidden-size 1024    --num-attention-heads 8     --seq-length 2048    --max-position-embeddings 2048    --num-experts 8    --moe-token-dispatcher-type alltoall    --disable-bias-linear)TRAINING_ARGS=(    --micro-batch-size 1     --global-batch-size 16     #--rampup-batch-size 16 16 5859375     --train-iters 5     --weight-decay 0.1     --adam-beta1 0.9     --adam-beta2 0.95     --init-method-std 0.006     --clip-grad 1.0     --bf16    --lr 6.0e-5     --lr-decay-style cosine     --min-lr 6.0e-6    --lr-warmup-fraction .001     --lr-decay-iters 430000 )MODEL_PARALLEL_ARGS=(\t--tensor-model-parallel-size 1 \t--pipeline-model-parallel-size 1     --expert-model-parallel-size 4      #--sequence-parallel )DATA_ARGS=(    #--data-path $DATA_PATH     --mock-data    --vocab-file $VOCAB_FILE     --merge-file $MERGE_FILE     --split 100,0,0)EVAL_AND_LOGGING_ARGS=(    --log-interval 1     --save-interval 10000     --eval-interval 1000     #-save $CHECKPOINT_PATH     #--load $CHECKPOINT_PATH     --eval-iters 0    #--tensorboard-dir $TENSORBOARD_LOGS_PATH )torchrun $&#123;DISTRIBUTED_ARGS[@]&#125; pretrain_gpt.py \\    $&#123;GPT_MODEL_ARGS[@]&#125; \\    $&#123;TRAINING_ARGS[@]&#125; \\    $&#123;MODEL_PARALLEL_ARGS[@]&#125; \\    $&#123;DATA_ARGS[@]&#125; \\    $&#123;EVAL_AND_LOGGING_ARGS[@]&#125;  \\    2&gt;&amp;1 | tee need/gpt.log     \n\n[ ] 一个脚本启动2个机器\n\nray常用命令\nray start --headray statusray stop\n","tags":["技术","多机训练"]},{"title":"并行策略","url":"/2023/11/20/%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/","content":"简介主要针对transformer模型架构，megatron-LM框架。\n输入input X维度 （batch_size, sequence, hidden_size) （b, s, h)\n资料\nNVIDIA/Megatron-LM   官网里面给出了经典的3篇文章\n常见的分布式并行策略 - OneFlow\n并行技术 | Colossal-AI (colossalai.org)\n数据并行DP从 b维度进行切分。要 all-reduce 互相传梯度。\nzero1 对于 optimizer states 的切分。每个gpu只维护一部分优化器状态，也只更新一部分参数。这样显存和计算都减少了。\n\n通信量并没有增加。Ring AllReduce就是分为两个步骤：reduce-scatter和 all-gather。\n通信量：$2\\times (N-1)  \\times  \\frac{K}{N} $    k是数据大小，N个GPU\n资料\nRing AllReduce简介 \n张量并行TP从h维度切分。attention层一个GPU放几个head。只要在g处做all-reduce加在一起就行。\n先做一个列并行，再做一个行并行。\n\n列并行:把权重A按照列来分割。\n\n行并行:把权重A按照行分割成两部分。\n\nsequence parallelsp从s维度切分。all-reduce变成all-gather和reduce-scatter。通信g是前向的all-gather，反向reduce-scatter，$\\hat{g}$相反。同样通信量也没有增加。\n具体是 layernorm 和 dropout 按sequence维度拆分。这2个层可以序列并行，因为layernorm是一个token一个token算的。\n\nsp也可以用到attention层，但会引入通信，适用场景是长序列输入。\n流水线并行Gpipe。算完前向再算反向。\n\n1F1B、交错流水线 。上面是1F1B，保持进行中的微批次数量里只有4个（与管道深度一样），这样相比Gpipe显存会更小。\nlayers 16，pp4配置下。1F1B划分：1-4，5-8，9-12，13-16。交错流水线可以1-2，9-10；3-4，11-12；5-6，13-14；7-8，15-16这样划分，可以理解为先把模型划分成几块（2块），再依次划分给GPU。这样需要更多轮1234才能算完（2次）一个前（反）向，但是时间也相应减小了，使得bubble占比更小。\n\nmoe专家并行基于 Transformer 的 MoE，即将 Transformer 中的 MLP 层替换为 MoE 层。其他操作没变。\n从数据并行到专家并行需要转化切分维度，做all-to-all操作。\n在megatron中，是用all-gather、reduce-scatter实现all-to-all功能的。\n\n\n[ ] SP\n\n","tags":["技术","并行策略"]},{"title":"硬件知识","url":"/2024/03/13/%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86/","content":"CPU取数据CPU依次从 cache、主存、磁盘中读取数据。cache中没有，如果主存中有，读取后会写入到cache中。但主存中也没有，则会用DMA从磁盘读取到主存中，而不是CPU直接读取磁盘。\nframe是物理内存一个块的大小，page是虚拟内存，一般二者相等。\n硬件NIC 网卡\n随机访问存储器RAM（Random Access Memory） /DDR/ HBM 都是内存\nPCIe switch  就是一个接口拓展，把硬件连接起来\nPCIe总线连接CPU和其他外设，比如GPU，网卡等\n资料\nPCIe（一） —— 基础概念与设备树 | Soul Orbit (r12f.com)\nNVLinkNVLink 是 GPU 和 CPU 之间的高速连接通道，第四代 NVLink 速度高达每秒 900GB/s，是PCIe 5.0 带宽的 7 倍多。\nRDMA跨机直接访问内存的技术，会减少开销\n资料\n【最新】Nvidia GPU互联技术全景图 - 知乎 (zhihu.com)\n","tags":["技术","硬件"]},{"title":"个人成长","url":"/2023/01/01/%E7%94%9F%E6%B4%BB/%E4%B8%AA%E4%BA%BA%E6%88%90%E9%95%BF/","content":"\n  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\n  \n    \n      \n      \n        请输入密码后查看\n      \n    \n  \n\n","tags":["生活","private"]},{"title":"经济学常识","url":"/2025/04/07/%E7%94%9F%E6%B4%BB/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%B8%B8%E8%AF%86/","content":"111\n经济常识\n这几天美国加关税，搞得很热闹。\n关税\n提高进口商品的价格，使本土生产的同类产品在价格上更具竞争力，从而为国内制造业提供了一定的保护。\n降息（加息），借钱和存钱的利率都会降低（增加）\n降息\n\n刺激经济 ：企业贷款成本降低，更愿意投资扩产；消费者贷款（如房贷、车贷）更便宜，促进消费。\n风险资产价格上升： 低利率环境通常会推动股市和其他风险资产的价格上涨，但同时也可能引发资产泡沫。\n通胀压力增加： 降息可能导致市场上货币供应量增加，如果供给跟不上需求，可能会引发通胀上升。\n\n加息\n\n减小通胀： 加息可以提高借贷成本，从而抑制投资和消费需求，防止通胀加剧。\n防范金融风险： 通过提高利率，可以降低过度杠杆化风险，防止资产泡沫形成。\n\n杠杆效应 是指利用借款来放大投资收益。然而，借款过多（即过度杠杆化）时，一旦市场出现不利变化，投资者或企业的偿债能力可能不足，导致违约和金融风险累积。低利率环境下，融资成本较低，企业和个人更容易通过借款来增加投资或消费，从而推动杠杆水平的不断提高。\n资产泡沫的成因： 在低利率环境下，投资者为了获得更高的回报，往往会投入大量资金到股市、房地产等资产市场，导致这些资产价格远远偏离其内在价值。\n通胀率通胀率通常反映的是一个国家经济中物价水平的平均上涨幅度。通胀并非越低越好，温和通胀（2-3%）通常被视为经济健康的标志。\n","tags":["生活","经济"]},{"title":"芙莉莲","url":"/2023/05/09/%E7%94%9F%E6%B4%BB/%E8%8A%99%E8%8E%89%E8%8E%B2/","content":"前言初中时看魔戒、霍比特人给我印象很深，很喜欢那种一行人冒险的故事（人类勇士，法师，矮人，精灵），自由的探险风景又很好。芙莉莲风格类似，同时富有深度，记录下感想。\n人是需要被肯定的活在社会中，总体也算善良正义。我们都希望自己的一生是有所贡献，值得肯定的呀。\n\n所以当他人愿意说出自己的事迹，不要吝啬赞美，要给出肯定啊。\n\n人的心灵是很脆弱的，需要一些支撑，要不然也不会出现那么多宗教。这里的女神大人，应该是希望有个客观的‘神“来评判自己的一生。\n心理暗示人们大都需要积极的心理暗示。但是就算没拔出勇者之剑，老子依旧要去干倒魔王的勇气真的好可贵！\n还是要专心做事吧，之前的种种并不代表什么，比如啥高考成绩、被谁谁谁肯定/否定等。做事的时候仅仅是在做这件事而已，不要给自己太多心理暗示，如一定能做成（做不成）。\n\n","tags":["生活","芙莉莲"]},{"title":"Megatron-LM","url":"/2023/10/19/%E6%8A%80%E6%9C%AF/%E7%BB%93%E6%9D%9F/Megatron-LM/","content":"运行英伟达的并行框架，很常用。\ngpt需要vocab，merges 2个文件，预处理数据，补充sh脚本里的参数，如GPUS_PER_NODE、tp、pp。\n运行命令：\nbash examples/pretrain_gpt.sh \n\ncheckpoint可以不保存，直接注释掉，—load、—save\n—mock-data 可以生成假数据，需要注释掉 —data-path \n\nA402024/7/30 用最新的megatron代码，最新的docker镜像，运行很顺利。\ndocker run -it --name ydshi_8_2 --gpus all --shm-size=64G -v /home/ydshi/data:/workspace nvcr.io/nvidia/pytorch:24.04-py3 /bin/bash\n用 Megatron-LM 训练一个 GPT-2 | Nólëbase (ayaka.io)   数据集来源\n显存不够CUDA out of memory，可以减小模型参数。\n​    —num-layers 12 \\​    —hidden-size 256  \n","tags":["技术","Megatron"]},{"title":"docker环境","url":"/2024/04/09/%E6%8A%80%E6%9C%AF/%E7%BB%93%E6%9D%9F/docker%E7%8E%AF%E5%A2%83/","content":"docker-v挂载数据，工作目录命名为workspace。\n直接用宿主机的空间和网络 —ipc=host —net=host 。（方便跨机）\ndocker run -it --name ydshi_loongserve --gpus all --ipc=host --net=host -v /nfs/ydshi:/workspace loongserve:latest /bin/bash\n创建完容器之后重新打开\ndocker start ydshidocker exec -it ydshi /bin/bash \n将容器保存为镜像，传输docker 镜像\ndocker commit abc123 my_image:latestdocker save -o my_image.tar my_image:latestscp my_image.tar root@192.168.0.1:/pathdocker load -i my_image.tar \n查看镜像        docker images   \n查看容器        docker ps \n删除镜像        docker rmi\n删除容器        docker rm\n容器中nvidia-smi 失效，尝试：\ndocker restart container           \nconda如果容器里面项目很多，也应该用conda管理环境\nconda create -n fast-dllm python=3.10           // 一定要指定python版本\ncuda环境pytorch，cuda要兼容。 \ncuda版本。\nnvcc --version\npytorch依赖的cuda版本\npython -c &quot;import torch; print(torch.version.cuda)&quot;\n心得vscode 可以打开服务器里面的文件， code XXX能直接打开文件。\nDev Containers 连接到容器内部，从而打开容器里面的文件进行跳转 （Attach to Running Container…） 。需要下载python拓展，选对解释器。\n\n","tags":["技术","docker"]},{"title":"博客使用","url":"/2022/09/02/%E6%8A%80%E6%9C%AF/%E7%BB%93%E6%9D%9F/%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8/","content":"介绍hexo+keep主题，部署在github的静态网站。 \n我需要的博客功能\n\n代码折叠，数学公式，简洁美观。\n\nGit Bash进入博客目录中（d:\\blog，d:\\blog\\source\\_posts都行）。\n常用命令：\nhexo clean  # 清除缓存hexo g      # 生成静态网页hexo s      # 启动服务器，本地查看hexo d      # 部署到Github\n加密\n用密码访问，在github仓库能看到。Hexo加密   \n\n放在_draft文件夹，就只能在本地查看，不会上传。\n\n\n图床typora+picgo-github图床，picgo要打开时间戳重命名。\ntypora的图片统一复制到本地文件夹，如typora/img。博客中的图片就手动上传。\n图片还是别压缩了。下载compress插件，搭配压缩网站（tinify.com)  。   \n数学公式如何在 hexo 中支持 Mathjax  实测简单好使\n浏览器需要安装插件：TeX All the Things\nTypora行内公式用单$, 行间就插入公式块。\ntips1修改仓库,这样hexo d好使点\nrepository: git@github.com:shiyandong/shiyandong.github.io.git#repository: https://github.com/shiyandong/shiyandong.github.io.git\ntips2git bash默认的框太丑了，也不能缩放。得集成到windows terminal\n","tags":["技术","blog"]},{"title":"深度学习基础","url":"/2023/12/01/%E6%8A%80%E6%9C%AF/%E7%BB%93%E6%9D%9F/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","content":"常用方法Cross Entropy Loss交叉熵主要用于度量两个概率分布间的差异性。标签值 y（分类时用one-hot编码，真实类别是1，其他为0）， 预测值 a，分类数n\n\n\\text { loss }=-\\sum_{j=1}^n y_j \\ln a_jsoftmax多类别分类的激活函数，将一组实数转化为表示概率分布的值。每个元素都在 0 到 1 之间，且所有元素的和为 1。\n给定输入向量 $z=\\left(z_1, z_2, \\ldots, z_k\\right)$ ，计算公式为:\n\n\\begin{aligned}\n\\sigma(z)_i=\\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}，i=1,2, \\ldots, k\n\\end{aligned}其中， $e$ 是自然对数的底， $k$ 是类别的数量。\nlarynorm\n用于加速训练的。对单个样本的不同特征做归一化操作，公式：\n\n\\begin{aligned}\n\\mu_i & =\\frac{1}{n} \\sum_{j=1}^n x_{i j} \\\\\n\\sigma_i^2 & =\\frac{1}{m} \\sum_{j=1}^m\\left(x_{i j}-\\mu_i\\right)^2 \\\\\n\\hat{x}_{i j} & =\\frac{x_{i j}-\\mu_i}{\\sqrt{\\sigma_i^2+\\epsilon}}\n\\end{aligned}\n\\begin{aligned}\n\n\n \\end{aligned}dropout用于防止神经网络过拟合。在训练过程中，随机地将一部分神经元的输出设置为零。\n资料\n03.2 交叉熵损失函数 - AI-EDU (microsoft.github.io)\n优化器深度学习的优化目标是最小化目标（损失）函数，就是反向传播算梯度从而更新参数。不同优化器区别在于q函数。\n待优化参数: $\\theta$ ，目标函数： $f(\\theta)$ ，学习率: $\\alpha$\n $\\mathrm{t}$ 时刻参数的梯度: $g_t=\\nabla f\\left(\\theta_t\\right)$\n优化通式：  $\\theta_t=\\theta_{t-1}-q\\left(g_t\\right)$\nSGDSGD的梯度下降过程，类似于一个小球从山坡上滚下，它的前进方向只与当前山坡的最大倾斜方向一致(梯度反方向)，每一个时刻的初速度都为０。\n\n\\begin{aligned}\n\n\\theta_t=\\theta_{t-1}-\\alpha * g_t\n\n \\end{aligned}SGD容易收敛到局部最优，在某些情况下可能被困在鞍点。\n引入 Momentum \n\n\\begin{aligned}\n\nm_t & =\\text { momentum } * m_{t-1}+\\alpha * g_t \\\\\n\\theta_t & =\\theta_{t-1}-m_t\n\n\\end{aligned}假设momentum =0.9,$ \\alpha=0.01$, 有:\n\n\\begin{aligned}\n& m_5=0.9 m_4+0.01 g_5 \\\\\n& m_4=0.9 m_3+0.01 g_4 \\\\\n& m_3=0.9 m_2+0.01 g_3 \\\\\n& m_2=0.9 m_1+0.01 g_2\n\\end{aligned}则: $m_5=0.01 *\\left(g_5+0.9 g_5+0.9^2 g_3+0.9^3 g_2+0.9^4 g_1\\right)$可以看到第5次更新的梯度包含了前4次的梯度，且是一个指数衰减的过程。\nSGD Momentum的梯度下降过程，前进方向由当前山坡的最大倾斜方向与之前的下降方向共同决定，小球具有初速度(动量)。\nAdam\n\\begin{aligned}\nm_t & =\\beta_1 m_{t-1}+\\left(1-\\beta_1\\right) g_t \\\\\n\\hat{m}_t & =\\frac{m_t}{1-\\beta_1^t} \\\\\nV_t & =\\beta_2 V_{t-1}+\\left(1-\\beta_2\\right) g_t^2 \\\\\n\\hat{V}_t & =\\frac{V_t}{1-\\beta_2^t} \\\\\n\\alpha_t & =\\frac{\\alpha}{\\sqrt{\\hat{V}_t}+\\epsilon} \\\\\n\\theta_t & =\\theta_{t-1}-\\alpha_t * \\hat{m}_t    \\\\\n\n\\end{aligned}二阶动量$V_t $ 是历史梯度平方和，度量历史更新频率。参数更新越频繁，二阶动量越大，学习率就越小。$\\hat{m}_t 、\\hat{V}_t$是偏差修正。\n资料\noptimizer优化器总结 - weber’s Blog (haiping.vip)\n反向传播就是求损失函数C对参数的梯度，利用梯度更新参数。\n反向计算梯度时，根据前向不同的计算公式，从而有相应的系数。\n资料 \nBack Propagation and Gradient Calculation in Deep Learning (hannlp.github.io)\n下篇 反向传播的微积分原理_哔哩哔哩_bilibili\n卷积神经网络(CNN)反向传播算法 - 刘建平Pinard - 博客园 (cnblogs.com)\nconv2d结构Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) \n这个卷积层参数：$64\\times3\\times9+64 = 1792$  \n一个通道的结果： 输入的3个通道各自经过一个卷积核再相加得到的，卷积核个数=输出通道*输入通道，每个通道还有有一个偏置。 \n资料\n卷积操作参数量和计算量       卷积核图示\n","tags":["技术","深度学习"]},{"title":"运动","url":"/2025/05/31/%E7%94%9F%E6%B4%BB/%E8%BF%90%E5%8A%A8/","content":"篮球把球转到横纹，投篮会更准。\n正常防守右手球员，用左手防。\n投篮力量有点不够，得把篮球贴紧手掌才行。\n打球时不要生气较劲，没意义，得找友善，素质好的人一起玩。\n眼镜打篮球时可以带上运动眼镜，右眼400，左眼425。\n\n跑步还是坐在位子上的时间太久了，应该抽空多走路，多跑步\n","tags":["生活","运动"]},{"title":"投机推理","url":"/2025/09/23/%E6%8A%80%E6%9C%AF/%E6%8A%95%E6%9C%BA%E6%8E%A8%E7%90%86/","content":"\n投机推理原始方法：target Model + draft model\n小模型快速生成多个候选 token ，大模型做一次forward并行验证。\n小模型占用显存和算力，带来额外负担。\n主流方法：Eagle\n基于大模型自身的中间层隐藏状态，直接预测未来多个 token。\n使用一个 lightweight 的 预测头 (predictor head) 来并行生成候选序列。\nn-gram \n通过在文本中查找重复出现的 n-gram（连续的n个词或字符）来进行预测。例如，如果 “the quick brown fox” 经常出现，那么当模型生成了 “the quick brown” 之后，它就会预测下一个词是 “fox”。用到的就是算法竞赛学的字典树（Trie），挺有意思。\n","tags":["技术","投机推理"]},{"title":"大模型基础知识","url":"/2023/11/01/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","content":"EmbeddingEmbedding层维度大小是 [v,h]，v是词汇表大小，h是hidden_size。每个token都对应词汇表中的一个索引，embedding后就是对应索引的那行。\n旋转位置编码（RoPE）“猫 追 狗”，”狗 追 猫”。如果没有位置信息，Attention 会认为这两句话的“追”字上下文完全一样，这显然是错误的。因此，必须用一种方法告诉模型每个 token 的位置。\nRoPE 不是 Token Embedding + 位置信息，而是对 Query ($q$) 和 Key ($k$) 向量进行“旋转” (Rotate)，实现了在 Attention 计算中只体现“相对位置”的效果。\n1 分组： RoPE 会把 $q$ 和 $k$ 向量的 embedding 维度两两一组 (e.g., $(d_1, d_2), (d_3, d_4), …$)。\n2 旋转矩阵： 对于一个 2D 向量 $(x, y)$，左乘旋转矩阵$R_{\\theta}$时，就是逆时针旋转 $\\theta$ 角度\n\nR_{\\theta} = \\begin{pmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{pmatrix}3 定义旋转  RoPE 定义了一个与绝对位置 $m$ 和维度 $i$ 相关的旋转角度 $\\theta_{m,i} = m \\cdot \\lambda_i$。通常 $\\lambda_i = 10000^{-2i/d}$。\n4 计算\n\n(q'_m)^T k'_n = (R_{m, \\theta_i} q_m)^T (R_{n, \\theta_i} k_n)=q_m^T R_{m-n, \\theta_i} k_n相对距离 $|m-n|$ 越大时，编码后的点积（Attention）会自然衰减。\nAttentionAttention能并行建模序列的全局依赖。\nMulti-Head 多头自注意力头，能在不同尺度上同时提取互补关系。d_model 变成 d_model / h。\n\n流程输入$X$通过参数矩阵 $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V$分别得到 $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$。\n$\\text{Attention  Score} = \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}$,  用（ Key 向量的维度的平方根 ）进行缩放，以避免点积结果过大。\n将分数进行$\\text{softmax}$ 归一化得到权重，拿权重乘对应的 $\\mathbf{V}$再求和，得到最终的output。\n\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}Attention maskAttention Mask  通过在 $\\text{softmax}$ 之前，将某些位置加上负无穷来实现的。\n\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}} + \\mathbf{M}\\right) \\mathbf{V}Causal Mask保持因果关系，上三角都是负无穷，在训练以及推理的prefill阶段会用到。\n\n\\mathbf{M}_{Causal} = \\begin{pmatrix}\n0 & -\\infty & -\\infty & -\\infty \\\\\n0 & 0 & -\\infty & -\\infty \\\\\n0 & 0 & 0 & -\\infty \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}MHA，GQA多头注意力（MHA），每个头都有一套 Q/K/V \nnum_attention_heads = num_key_value_heads = h。。\nGrouped-Query Attention 分组查询注意力，h 个 query会分组（ h / h_kv ），同一组的多个 Q 共享一套 K/V。\nnum_key_value_heads = h_kv &lt; h。 \nFeed-Forward Network (FFN)FFN由两层线性变换和中间的非线性激活函数组成。\n\nFFN(x) = \\text{Linear}_2(\\text{Activation}(\\text{Linear}_1(x)))激活函数（如ReLU、GELU）引入了非线性，极大地增强了模型的表示能力。\nAdd &amp; Norm残差连接和 LayerNorm\n\ny=x+f(x)资料\n图解RoPE旋转位置编码及其特性 - 知乎\nTransformer - YouTube   讲的很清楚\n","tags":["技术","大模型基础知识"]},{"title":"大模型推理","url":"/2024/04/05/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86/","content":"整理一些大模型推理相关知识\n推理流程推理包含2个阶段Prefill , Decode。\nPrefill可以并行处理输入，生成KV cache缓存以及第一个output token，是computation-bound。\nDecode基于KV cache自回归的生成后续token，一次迭代只能生成一个token，是memory-bound。\nKV cachedecode生成下一个token时，需要使用当前的Q(Query)，并结合之前token的KV（Key-Value）。\n推理指标TTFT、TBT、Throughput、Goodput\n推理架构PD分离\n将 Prefill 阶段和 Decode 阶段拆分到不同实例上执行，可以降低 TBT延迟，因为Decode 不再被长序列的 Prefill 阻塞。但请求不平衡时GPU利用率低，Throughput低。\nPD聚合\n通常与 Chunked Prefill 搭配使用。把Prefill 拆分成多个 chunk，在每个 chunk 内同时包含一部分 Prefill 和 Decode，能减少二者之间的干扰。chunksize 调大，延迟一般就会增加。\nvLLM关键参数\nserver端\n--max-num-batched-tokens 2048 \\     \\\\batch的最多token数--max-num-seqs 256 \\                 \\\\batch的最多请求数量\nclient端\n--ignore-eos \\--burstiness 100 \\     请求分布\n资料\nLLM推理入门指南②：深入解析KV缓存 (qq.com)\nHow a Transformer works at inference vs training time - YouTube\n","tags":["技术","推理"]},{"title":"GPU profile","url":"/2025/03/23/%E6%8A%80%E6%9C%AF/GPU%20profile/","content":"\n111\nNsight systemnsys profile -o report.nsys-rep --trace-fork-before-exec=true --cuda-graph-trace=node --force-overwrite true \\\\\\ 标记torch.cuda.nvtx.range_push(&quot;Prefill&quot;)torch.cuda.nvtx.range_pop()\n可以exprot成数据库，然后用SQL语句对数据库进行分析。\nNsight system还是功能更强大点。\nPytorch profilerwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, use_cuda=True) as prof:\\\\标记with record_function(&quot;Prefill&quot;):\n","tags":["技术","profile"]},{"title":"好用的工具","url":"/2025/03/16/%E7%94%9F%E6%B4%BB/%E5%A5%BD%E7%94%A8%E7%9A%84%E5%B7%A5%E5%85%B7/","content":"\n应用Clash Verge  vpn\nlistray  搜索工具\nSnipaste 截图放在桌面上\nzotero 读论文\n欧路词典 查单词\nVScode 写代码\nTypora Notion 记笔记\n设备能用无线就尽量无线，无线鼠标、无线键盘。\n投屏笔记本电脑是可以盖上再投屏的\n设置：盖上盖子将使我的电脑，不执行任何操作\n","tags":["生活","工具"]}]